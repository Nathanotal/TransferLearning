{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install torch\n",
    "# %pip install torchvision\n",
    "# %pip install helper\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch as th\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from xml.dom import minidom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "Maybe normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as tttttt\n",
    "import random\n",
    "\n",
    "random.seed(5)\n",
    "np.random.seed(100)\n",
    "\n",
    "def open_image(path): # https://jovian.ai/aakashns/transfer-learning-pytorch\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, subset):\n",
    "        self.standardImagesAdded = 0\n",
    "        self.length = len(subset.dataset)\n",
    "        self.subset = subset # th.utils.data.ConcatDataset([subset, subset]) # Add data\n",
    "        self.transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                            # transforms.RandomRotation(degrees=(-30, 30)),\n",
    "                                            # transforms.Resize(255),\n",
    "                                            # transforms.CenterCrop(224), \n",
    "                                            transforms.ToTensor(),\n",
    "                                            ]) # normalization ABC\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if ((self.standardImagesAdded % self.length*2) < self.length): # Add standard images\n",
    "            self.standardImagesAdded += 1\n",
    "            return self.subset[index]\n",
    "        \n",
    "        # Add augmented images\n",
    "        x, y = self.subset[index]\n",
    "        t = tttttt.ToPILImage()\n",
    "        x = self.transform(t(x)) \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, binary=False, limit=1, train=True):\n",
    "        super().__init__()\n",
    "        self.size=224\n",
    "        self.files = self.initPaths(limit, train)\n",
    "        self.binary = binary\n",
    "        self.train = train\n",
    "        self.classes = self.initClasses() if not binary else self.initClassesCatDog()\n",
    "        # TODO: fix transforms better\n",
    "        self.transform = self.getTransform()\n",
    "        self.augTransform = self.getAugTransform()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i, stats=False):\n",
    "        path = \"./data/images/\" + self.files[i]\n",
    "        if stats:\n",
    "            img = open_image(path)\n",
    "        else:\n",
    "            if self.files[i][0:3] == \"AUG\":\n",
    "                img = self.augTransform(open_image(\"./data/images/\" + self.files[i][3:]))\n",
    "            else:\n",
    "                img = self.transform(open_image(path))\n",
    "        class_idx = self.classes.index(self.getClass(self.files[i]))\n",
    "\n",
    "        return img, class_idx\n",
    "    \n",
    "    def getTransform(self):\n",
    "        return transforms.Compose([transforms.Resize(255),\n",
    "                                    transforms.CenterCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ]) # normalization ABC\n",
    "        \n",
    "    def getAugTransform(self):\n",
    "        return transforms.Compose([transforms.RandomHorizontalFlip(p = 0.7), \n",
    "                                    transforms.RandomRotation(degrees=(-30,30)),\n",
    "                                    transforms.Resize(255),\n",
    "                                    transforms.CenterCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ]) # normalization ABC\n",
    "\n",
    "    \n",
    "    def getCenterBoundingBox(self, fileName):\n",
    "        try:\n",
    "            file = minidom.parse('./annotations/xmls/'+fileName.replace(\".jpg\", '.xml'))\n",
    "            xmin = file.getElementsByTagName('xmin')[0].firstChild.data\n",
    "            xmax = file.getElementsByTagName('xmax')[0].firstChild.data\n",
    "            ymin = file.getElementsByTagName('ymin')[0].firstChild.data\n",
    "            ymax = file.getElementsByTagName('ymax')[0].firstChild.data\n",
    "            return int(xmin), int(xmax), int(ymin), int(ymax)\n",
    "        except:\n",
    "            return 0, 0, 0, 0\n",
    "    \n",
    "    \n",
    "    def getStats(self):\n",
    "        avgImage = th.Tensor(np.zeros((self.size, self.size, 3)))\n",
    "        \n",
    "        for i in range(self.__len__()):\n",
    "            img,_ = self.__getitem__(i, stats=True)\n",
    "            t = transforms.Compose([transforms.Resize(255),\n",
    "                                    transforms.CenterCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    ]) \n",
    "            avgImage += t(img)\n",
    "            \n",
    "        \n",
    "        avgImage /= self.__len__()\n",
    "        \n",
    "        mean = []\n",
    "        std = []\n",
    "        for dimension in enumerate(avgImage):\n",
    "            print(dimension)\n",
    "            mean.append(th.mean(dimension))\n",
    "            std.append(th.std(dimension))\n",
    "        \n",
    "        print(mean, std)\n",
    "            \n",
    "        \n",
    "        # return [mean[1], mean[2], mean[3]], [std[1], std[2], std[3]]\n",
    "        \n",
    "    def getCropStats(self):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        n = 0\n",
    "        x_li = []\n",
    "        y_li = []\n",
    "        \n",
    "        for file in self.files:\n",
    "            xmin, xmax, ymin, ymax = self.getCenterBoundingBox(file)\n",
    "            if (xmin != 0) and (xmax != 0) and (ymin != 0) and (ymax != 0):\n",
    "                n += 1\n",
    "                \n",
    "                x += (xmax + xmin)/2\n",
    "                y += (ymax + ymin)/2\n",
    "                \n",
    "                x_li.append(x)\n",
    "                y_li.append(y)\n",
    "                \n",
    "        print(\"Average x:\", x/n, \"Average y:\", y/n)\n",
    "        \n",
    "        x_np = np.asarray(x_li)\n",
    "        y_np = np.asarray(y_li)\n",
    "        \n",
    "        x_std = np.std(x_np)\n",
    "        y_std = np.std(y_np)\n",
    "        print(\"STD x:\", x_std, \"STD y:\", y_std)\n",
    "            \n",
    "            \n",
    "    # Classifies data into Cat/Dog or into one of the 37 classes of breeds\n",
    "    def getClass(self, fileName):\n",
    "        self.getCenterBoundingBox(fileName)\n",
    "        if self.binary:\n",
    "            return self.getClassCatDog(fileName)\n",
    "        else:\n",
    "            nameList = fileName.split(\"_\")\n",
    "            name = \"\"\n",
    "            for namePart in nameList:\n",
    "                if \".jpg\" not in namePart:\n",
    "                    name += namePart\n",
    "            return name\n",
    "        \n",
    "    def initClasses(self):\n",
    "        classList = []\n",
    "        for fname in self.files:\n",
    "            theClass = self.getClass(fname)\n",
    "            if theClass not in classList:\n",
    "                classList.append(theClass)\n",
    "        return classList\n",
    "    \n",
    "    def getClassCatDog(self, fileName):\n",
    "       if fileName[0].isupper():\n",
    "           return \"cat\"\n",
    "       else:\n",
    "           return \"dog\"\n",
    "    \n",
    "    def initClassesCatDog(self):\n",
    "        classList = []\n",
    "        for fname in self.files:\n",
    "            theClass = self.getClass(fname)\n",
    "            if theClass not in classList:\n",
    "                classList.append(theClass)\n",
    "        return [\"cat\", \"dog\"]\n",
    "    \n",
    "    \n",
    "    def addAugmented(self, jpgList):\n",
    "        newJpgList = jpgList.copy()\n",
    "        for jpg in jpgList: # Doubles the ammount of data \n",
    "            newJpgList.append(\"AUG\"+jpg)\n",
    "\n",
    "        return newJpgList\n",
    "    \n",
    "    def initPaths(self, limit, train):\n",
    "        files = os.listdir(\"./data/images/\")\n",
    "        jpgList = []\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                jpgList.append(file)\n",
    "        \n",
    "        random.shuffle(jpgList)\n",
    "        \n",
    "        if train:\n",
    "            jpgList = jpgList[:int(len(jpgList)*0.7)]\n",
    "            jpgList = self.addAugmented(jpgList)\n",
    "            random.shuffle(jpgList)\n",
    "        else:\n",
    "            jpgList = jpgList[int(len(jpgList)*0.7):]\n",
    "        \n",
    "        return jpgList[0:int(len(jpgList)*limit)]\n",
    "    \n",
    "\n",
    "test = MyDataset(False)\n",
    "# test.getCropStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c5808a8359e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-c5808a8359e8>\u001b[0m in \u001b[0;36minitDataset\u001b[1;34m(batch_size, binary)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# train, test = random_split(dataset, [int(0.85*len(dataset))+1, int(0.15*len(dataset))])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestValDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestValDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# print(len(train), len(valid), len(test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[1;34m(dataset, lengths, generator)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def initDataset(batch_size, binary): #batch size affects computation time\n",
    "    train = MyDataset(binary, train=True)\n",
    "    testValDataset = MyDataset(binary, train=False)\n",
    "    # Train/Validation/Test split. Current: 70/15/15\n",
    "\n",
    "    # train, test = random_split(dataset, [int(0.85*len(dataset))+1, int(0.15*len(dataset))])\n",
    "    valid, test = random_split(testValDataset, [int((0.5)*len(testValDataset)), int((0.5)*len(testValDataset))]) \n",
    "    \n",
    "    # print(len(train), len(valid), len(test))\n",
    "    # print(len(train)+ len(valid)+ len(test), len(dataset))\n",
    "    \n",
    "    # Enable augementation for the training dataset\n",
    "    # augmentedDataset = AugmentedDataset(train)\n",
    "    \n",
    "    train_loader = th.utils.data.DataLoader(train,\n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=True)\n",
    "    \n",
    "    test_loader = th.utils.data.DataLoader(test,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    valid_loader = th.utils.data.DataLoader(valid,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    \n",
    "    return train_loader , test_loader, valid_loader\n",
    "\n",
    "train_loader, test_loader, valid_loader = initDataset(batch_size=32, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 0\n",
    "# n2 = 0\n",
    "# for batch_idx, batch in enumerate(train_loader):\n",
    "#     n += 32\n",
    "#     print(n)\n",
    "    \n",
    "#     if n == 8000:\n",
    "#         images = batch[0]\n",
    "#         labels = batch[1]\n",
    "#         for i in range(len(images)):\n",
    "#             n2+=1\n",
    "#             plt.imshow(images[i].permute(1, 2, 0))\n",
    "#             plt.show()\n",
    "#             print(labels[i])\n",
    "#             if n2 == 1:\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tv.models.resnet18(progress = True, pretrained=True)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Set requires_grad to false for every layer\n",
    "moreLayers = True\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "modules = model.named_modules()\n",
    "\n",
    "for i in modules:\n",
    "    if isinstance(i[1], th.nn.BatchNorm2d):\n",
    "        if \"layer4\" in i[0] and \"bn\" in i[0]:\n",
    "            i[1].momentum = 0.5\n",
    "    \n",
    "# Replace the last layer of the pretrained model with our own:\n",
    "# This should theoretically only set the last layer to requires_grad = True, since it is the default setting\n",
    "model.fc = th.nn.Linear(model.fc.in_features, 37) # 37 if not binary\n",
    "\n",
    "# Set the second layer to requires_grad = True\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Replace more layers:\n",
    "# TODO: implement\n",
    "\n",
    "# Examine different learning rates / rate schedulers\n",
    "# TODO: implement\n",
    "\n",
    "# Apply data augmentation during training (flip, small rotations, crops, small size scaling)\n",
    "# TODO: implement\n",
    "\n",
    "# Effect of fine-tuning or not the batch-norm parameters and updating the estimate of the batch \n",
    "# mean and standard deviations on the final performance on the new dataset.\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on our dataset\n",
    "def train_model(model, train_loader, valid_loader, epochs=5, lr=10**-3, weight_decay=0.0, sheduler_gamma=0.9):\n",
    "    # Define the loss function\n",
    "    criterion = th.nn.CrossEntropyLoss()\n",
    "    # Define the optimizer\n",
    "    optimizer = th.optim.Adam(model.fc.parameters(), lr=lr, weight_decay=weight_decay) # filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    \n",
    "    # Set different learning rates for different layers\n",
    "    optimizer.add_param_group({'params': model.layer4.parameters(), 'lr': lr/10})\n",
    "    \n",
    "    # Train the model\n",
    "    scheduler = th.optim.lr_scheduler.ExponentialLR(optimizer, gamma=sheduler_gamma)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train() #trains model\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad() # Reset the gradients, maybe we should not do this\n",
    "            loss.backward() # Compute the gradients\n",
    "            optimizer.step() # Update the weights\n",
    "        # Validation\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        with th.no_grad(): # Disables tracking of calculations required to calculate gradients\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_idx, batch in enumerate(valid_loader):\n",
    "                images = batch[0]\n",
    "                labels = batch[1]\n",
    "                #print(labels)\n",
    "                outputs = model(images)\n",
    "                #print(outputs)\n",
    "                _, predicted = th.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            print('Learning rate: {} \\n'.format(lr))\n",
    "            print('Accuracy of the network on the validation images: {} %'.format(100 * correct / total))\n",
    "            \n",
    "    return model\n",
    "\n",
    "# new = train_model(model, train_loader, valid_loader, epochs=20, lr=10**-3 , weight_decay=0.0)\n",
    "new = train_model(model, train_loader, valid_loader, epochs=5, lr=10**-2, weight_decay=0.0, sheduler_gamma=0.5)\n",
    "#new = train_model(model, train_loader, valid_loader, epochs=5, lr=10**-3, weight_decay=0.0)\n",
    "#new = train_model(model, train_loader, valid_loader, epochs=5, lr=10**-4, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2 = train_model(model, train_loader, valid_loader, epochs=10, lr=10**-3, weight_decay=0.0,sheduler_gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2 = train_model(model, train_loader, valid_loader, epochs=10, lr=10**-3, weight_decay=0.0, sheduler_gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3 = train_model(model, train_loader, valid_loader, epochs=20, lr=10**-4, weight_decay=0.0, sheduler_gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestones:\n",
    "## Grade E:\n",
    "- [x] Achieve >99% on binary classification\n",
    "- [ ] Achieve >95% on multi-class classification\n",
    "- [x] Examine fine tuning more layers\n",
    "- [x] Examine different learning rates\n",
    "- [x] Examine data augmentation\n",
    "- [-] Fine tune batch-norm\n",
    "\n",
    "## Grade A:\n",
    "### Decrease the percentage of labelled data: \n",
    "- [ ] 50 %\n",
    "- [ ] 10 %\n",
    "- [ ] 1 %\n",
    "- [ ] Implement Pseudo-labelling\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89627e806f793b932dfe80791ab48950fda4fb8e20d46d5d1c8fbf2fdce875b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
