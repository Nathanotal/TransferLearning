{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install torch\n",
    "# %pip install torchvision\n",
    "# %pip install helper\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch as th\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from xml.dom import minidom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "Maybe normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as tttttt\n",
    "import random\n",
    "\n",
    "random.seed(5)\n",
    "np.random.seed(100)\n",
    "\n",
    "def initPathsBase():\n",
    "        files = os.listdir(\"./data/images/\")\n",
    "        jpgList = []\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                jpgList.append(file)\n",
    "        \n",
    "        random.shuffle(jpgList)\n",
    "        \n",
    "        return jpgList\n",
    "\n",
    "def initClasses(binary):\n",
    "    files = initPathsBase()\n",
    "    classList = []\n",
    "    for fname in files:\n",
    "        theClass = getClass(binary, fname)\n",
    "        if theClass not in classList:\n",
    "            classList.append(theClass)\n",
    "    return classList\n",
    "    \n",
    "    \n",
    "def initClassesCatDog(binary):\n",
    "    files = initPathsBase()\n",
    "    classList = []\n",
    "    for fname in files:\n",
    "        fname = cleanFileName(fname)\n",
    "        theClass = getClass(binary, fname)\n",
    "        if theClass not in classList:\n",
    "            classList.append(theClass)\n",
    "    return [\"cat\", \"dog\"]\n",
    "\n",
    "def getClassCatDog(fileName):\n",
    "    if fileName[0].isupper():\n",
    "        return \"cat\"\n",
    "    else:\n",
    "        return \"dog\"\n",
    "    \n",
    "def cleanFileName(name):\n",
    "        if name[0:3] == \"AUG\":\n",
    "           return name[3:]\n",
    "        return name\n",
    "    \n",
    "    # Classifies data into Cat/Dog or into one of the 37 classes of breeds\n",
    "def getClass(binary, fileName):\n",
    "    fileName = cleanFileName(fileName)\n",
    "    # getCenterBoundingBox(fileName)\n",
    "    if binary:\n",
    "        return getClassCatDog(fileName)\n",
    "    else:\n",
    "        nameList = fileName.split(\"_\")\n",
    "        name = \"\"\n",
    "        for namePart in nameList:\n",
    "            if \".jpg\" not in namePart:\n",
    "                name += namePart\n",
    "        return name\n",
    "\n",
    "    \n",
    "def open_image(path): # https://jovian.ai/aakashns/transfer-learning-pytorch\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "    \n",
    "def getDataLists():\n",
    "    files = initPathsBase()\n",
    "    return files[:int(len(files)*0.7)], files[int(len(files)*0.7):]\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, binary=False, limit=1, train=True, paths=None):\n",
    "        super().__init__()\n",
    "        self.size=224\n",
    "        self.files = self.initPaths(limit=limit, train=train, li=paths) # self.initPaths(limit, train)\n",
    "        self.binary = binary\n",
    "        self.train = train\n",
    "        self.classes = initClasses(binary) if not binary else initClassesCatDog(binary)\n",
    "        # TODO: fix transforms better\n",
    "        self.transform = self.getTransform()\n",
    "        self.augTransform = self.getAugTransform()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i, stats=False):\n",
    "        path = \"./data/images/\" + self.files[i]\n",
    "        if stats:\n",
    "            img = open_image(path)\n",
    "        else:\n",
    "            if self.files[i][0:3] == \"AUG\":\n",
    "                img = self.augTransform(open_image(\"./data/images/\" + self.files[i][3:]))\n",
    "            else:\n",
    "                img = self.transform(open_image(path))\n",
    "        class_idx = self.classes.index(self.getClass(self.files[i]))\n",
    "\n",
    "        return img, class_idx\n",
    "    \n",
    "    def getClass(self, fileName):\n",
    "        return getClass(self.binary, fileName)\n",
    "    \n",
    "    def getTransform(self):\n",
    "        return transforms.Compose([transforms.Resize(255),\n",
    "                                    transforms.CenterCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ]) # normalization ABC\n",
    "        \n",
    "    def getAugTransform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p = 0.7), \n",
    "                                    transforms.RandomRotation(degrees=(-30,30)),\n",
    "                                    transforms.Resize(255),\n",
    "                                    transforms.CenterCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ]) # normalization ABC\n",
    "\n",
    "    \n",
    "    def getCenterBoundingBox(self, fileName):\n",
    "        try:\n",
    "            file = minidom.parse('./annotations/xmls/'+fileName.replace(\".jpg\", '.xml'))\n",
    "            xmin = file.getElementsByTagName('xmin')[0].firstChild.data\n",
    "            xmax = file.getElementsByTagName('xmax')[0].firstChild.data\n",
    "            ymin = file.getElementsByTagName('ymin')[0].firstChild.data\n",
    "            ymax = file.getElementsByTagName('ymax')[0].firstChild.data\n",
    "            return int(xmin), int(xmax), int(ymin), int(ymax)\n",
    "        except:\n",
    "            return 0, 0, 0, 0\n",
    "    \n",
    "    \n",
    "    def getStats(self):\n",
    "        avgImage = th.Tensor(np.zeros((self.size, self.size, 3)))\n",
    "        \n",
    "        for i in range(self.__len__()):\n",
    "            img,_ = self.__getitem__(i, stats=True)\n",
    "            t = transforms.Compose([transforms.Resize(255),\n",
    "                                    transforms.CenterCrop(224), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    ]) \n",
    "            avgImage += t(img)\n",
    "            \n",
    "        \n",
    "        avgImage /= self.__len__()\n",
    "        \n",
    "        mean = []\n",
    "        std = []\n",
    "        for dimension in enumerate(avgImage):\n",
    "            print(dimension)\n",
    "            mean.append(th.mean(dimension))\n",
    "            std.append(th.std(dimension))\n",
    "        \n",
    "        print(mean, std)\n",
    "            \n",
    "        \n",
    "        # return [mean[1], mean[2], mean[3]], [std[1], std[2], std[3]]\n",
    "        \n",
    "    def getCropStats(self):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        n = 0\n",
    "        x_li = []\n",
    "        y_li = []\n",
    "        \n",
    "        for file in self.files:\n",
    "            xmin, xmax, ymin, ymax = self.getCenterBoundingBox(file)\n",
    "            if (xmin != 0) and (xmax != 0) and (ymin != 0) and (ymax != 0):\n",
    "                n += 1\n",
    "                \n",
    "                x += (xmax + xmin)/2\n",
    "                y += (ymax + ymin)/2\n",
    "                \n",
    "                x_li.append(x)\n",
    "                y_li.append(y)\n",
    "                \n",
    "        print(\"Average x:\", x/n, \"Average y:\", y/n)\n",
    "        \n",
    "        x_np = np.asarray(x_li)\n",
    "        y_np = np.asarray(y_li)\n",
    "        \n",
    "        x_std = np.std(x_np)\n",
    "        y_std = np.std(y_np)\n",
    "        print(\"STD x:\", x_std, \"STD y:\", y_std)\n",
    "            \n",
    "    \n",
    "    def addAugmented(self, jpgList):\n",
    "        newJpgList = jpgList.copy()\n",
    "        for jpg in jpgList: # Doubles the ammount of data \n",
    "            newJpgList.append(\"AUG\"+jpg)\n",
    "\n",
    "        return newJpgList\n",
    "    \n",
    "    def initPaths(self, limit, train, li):\n",
    "        # files = os.listdir(\"./data/images/\")\n",
    "        # jpgList = []\n",
    "        \n",
    "        # for file in files:\n",
    "        #     if file.endswith(\".jpg\"):\n",
    "        #         jpgList.append(file)\n",
    "        \n",
    "        # random.shuffle(jpgList)\n",
    "        jpgList = li\n",
    "        if train:\n",
    "            # jpgList = jpgList[:int(len(jpgList)*0.7)]\n",
    "            jpgList = self.addAugmented(li)\n",
    "            random.shuffle(jpgList)\n",
    "        else:\n",
    "            # jpgList = jpgList[int(len(jpgList)*0.7):]\n",
    "            pass\n",
    "        \n",
    "        return jpgList[0:int(len(jpgList)*limit)]\n",
    "    \n",
    "\n",
    "# test = MyDataset(False)\n",
    "# test.getCropStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def initDataset(batch_size, binary): #batch size affects computation time\n",
    "    trainPaths, testValPaths = getDataLists()\n",
    "    train = MyDataset(binary, train=True, paths=trainPaths)\n",
    "    testValDataset = MyDataset(binary, train=False, paths=testValPaths)\n",
    "    testValDataset.classes = train.classes # WTF\n",
    "    # Train/Validation/Test split. Current: 70/15/15\n",
    "\n",
    "    # train, test = random_split(dataset, [int(0.85*len(dataset))+1, int(0.15*len(dataset))])\n",
    "    valid, test = random_split(testValDataset, [int(0.5*len(testValDataset))+1, int(0.5*len(testValDataset))]) \n",
    "    \n",
    "    # print(len(train), len(valid), len(test))\n",
    "    # print(len(train)+ len(valid)+ len(test), len(dataset))\n",
    "    \n",
    "    # Enable augementation for the training dataset\n",
    "    # augmentedDataset = AugmentedDataset(train)\n",
    "    \n",
    "    train_loader = th.utils.data.DataLoader(train,\n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=True)\n",
    "    \n",
    "    test_loader = th.utils.data.DataLoader(test,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    valid_loader = th.utils.data.DataLoader(valid,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    \n",
    "    return train_loader , test_loader, valid_loader, train\n",
    "\n",
    "train_loader, test_loader, valid_loader, train = initDataset(batch_size=32, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 0\n",
    "# n2 = 0\n",
    "# for batch_idx, batch in enumerate(train_loader):\n",
    "#     n += 32\n",
    "#     if n%320 == 0:\n",
    "#         print(n)\n",
    "    \n",
    "#     if n == 64:\n",
    "#         images = batch[0]\n",
    "#         labels = batch[1]\n",
    "#         for i in range(len(images)):\n",
    "#             n2+=1\n",
    "#             plt.imshow(images[i].permute(1, 2, 0))\n",
    "#             plt.show()\n",
    "#             if n2 == 1:\n",
    "#                 break\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tv.models.resnet18(progress = True, pretrained=True)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Set requires_grad to false for every layer\n",
    "moreLayers = True\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "modules = model.named_modules()\n",
    "\n",
    "for i in modules:\n",
    "    if isinstance(i[1], th.nn.BatchNorm2d):\n",
    "        if \"layer4\" in i[0] and \"bn\" in i[0]:\n",
    "            i[1].momentum = 0.2\n",
    "    \n",
    "# Replace the last layer of the pretrained model with our own:\n",
    "# This should theoretically only set the last layer to requires_grad = True, since it is the default setting\n",
    "model.fc = th.nn.Linear(model.fc.in_features, 37) # 37 if not binary\n",
    "\n",
    "# Set the second layer to requires_grad = True\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Replace more layers:\n",
    "# TODO: implement\n",
    "\n",
    "# Examine different learning rates / rate schedulers\n",
    "# TODO: implement\n",
    "\n",
    "# Apply data augmentation during training (flip, small rotations, crops, small size scaling)\n",
    "# TODO: implement\n",
    "\n",
    "# Effect of fine-tuning or not the batch-norm parameters and updating the estimate of the batch \n",
    "# mean and standard deviations on the final performance on the new dataset.\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 324/324 [28:33<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001 \n",
      "\n",
      "Accuracy of the network on the validation images: 89.72046889089269 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 324/324 [26:44<00:00,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001 \n",
      "\n",
      "Accuracy of the network on the validation images: 90.892696122633 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 34/324 [03:54<31:34,  6.53s/it]"
     ]
    }
   ],
   "source": [
    "# Train the model on our dataset\n",
    "def train_model(model, train_loader, valid_loader, epochs=5, lr=10**-3, weight_decay=0.0, sheduler_gamma=0.9):\n",
    "    # Define the loss function\n",
    "    criterion = th.nn.CrossEntropyLoss()\n",
    "    # Define the optimizer\n",
    "    optimizer = th.optim.Adam(model.fc.parameters(), lr=lr, weight_decay=weight_decay) # filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    \n",
    "    # Set different learning rates for different layers\n",
    "    optimizer.add_param_group({'params': model.layer4.parameters(), 'lr': lr/10})\n",
    "    \n",
    "    # Train the model\n",
    "    scheduler = th.optim.lr_scheduler.ExponentialLR(optimizer, gamma=sheduler_gamma)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train() #trains model\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "            images = batch[0]\n",
    "            labels = batch[1]\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad() # Reset the gradients, maybe we should not do this\n",
    "            loss.backward() # Compute the gradients\n",
    "            optimizer.step() # Update the weights\n",
    "        # Validation\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        with th.no_grad(): # Disables tracking of calculations required to calculate gradients\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_idx, batch in enumerate(valid_loader):\n",
    "                images = batch[0]\n",
    "                labels = batch[1]\n",
    "                #print(labels)\n",
    "                outputs = model(images)\n",
    "                #print(outputs)\n",
    "                _, predicted = th.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            print('Learning rate: {} \\n'.format(lr))\n",
    "            print('Accuracy of the network on the validation images: {} %'.format(100 * correct / total))\n",
    "            # correct = 0\n",
    "            # total = 0\n",
    "            # for batch_idx, batch in enumerate(train_loader):\n",
    "            #     images = batch[0]\n",
    "            #     labels = batch[1]\n",
    "            #     #print(labels)\n",
    "            #     outputs = model(images)\n",
    "            #     #print(outputs)\n",
    "            #     _, predicted = th.max(outputs.data, 1)\n",
    "            #     total += labels.size(0)\n",
    "            #     correct += (predicted == labels).sum().item()\n",
    "            # print('Accuracy of the network on the train images: {} %'.format(100 * correct / total))\n",
    "            \n",
    "    return model\n",
    "\n",
    "# new = train_model(model, train_loader, valid_loader, epochs=20, lr=10**-3 , weight_decay=0.0)\n",
    "new = train_model(model, train_loader, valid_loader, epochs=5, lr=10**-3, weight_decay=0.0, sheduler_gamma=0.5)\n",
    "#new = train_model(model, train_loader, valid_loader, epochs=5, lr=10**-3, weight_decay=0.0)\n",
    "#new = train_model(model, train_loader, valid_loader, epochs=5, lr=10**-4, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "n = 0\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    images = batch[0]\n",
    "    labels = batch[1]\n",
    "    #print(labels)\n",
    "    outputs = model(images)\n",
    "    #print(outputs)\n",
    "    _, predicted = th.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    n+=1\n",
    "    for i in range(len(images)):\n",
    "            \n",
    "            plt.imshow(images[i].permute(1, 2, 0))\n",
    "            plt.title(\"Predicted: \" + str(train.classes[predicted[i].item()]) + \"\\nActual: \" + str(train.classes[labels[i].item()]), fontsize=15, color='green' if predicted[i] == labels[i] else 'red', fontweight='bold')\n",
    "            plt.show()\n",
    "    if n == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2 = train_model(model, train_loader, valid_loader, epochs=10, lr=10**-3, weight_decay=0.0,sheduler_gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2 = train_model(model, train_loader, valid_loader, epochs=10, lr=10**-3, weight_decay=0.0, sheduler_gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3 = train_model(model, train_loader, valid_loader, epochs=20, lr=10**-4, weight_decay=0.0, sheduler_gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestones:\n",
    "## Grade E:\n",
    "- [x] Achieve >99% on binary classification\n",
    "- [ ] Achieve >95% on multi-class classification\n",
    "- [x] Examine fine tuning more layers\n",
    "- [x] Examine different learning rates\n",
    "- [x] Examine data augmentation\n",
    "- [-] Fine tune batch-norm\n",
    "\n",
    "## Grade A:\n",
    "### Decrease the percentage of labelled data: \n",
    "- [ ] 50 %\n",
    "- [ ] 10 %\n",
    "- [ ] 1 %\n",
    "- [ ] Implement Pseudo-labelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo labelling\n",
    "\n",
    "# Reduce the amount of labels\n",
    "\n",
    "# Train a model on the reduced dataset\n",
    "\n",
    "# Use the model to generate pseudo labels\n",
    "\n",
    "# Train a model on the pseudo labels and the original dataset\n",
    "\n",
    "# Implement ensambling? Implement Pretrain?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89627e806f793b932dfe80791ab48950fda4fb8e20d46d5d1c8fbf2fdce875b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
